import os
import re
import time

from .build_model import APIModel

try:
    from health_bench.vllm_sampler import VLLMSampler
except Exception:
    VLLMSampler = None


api_url = os.getenv("VERIF_API_URL", "http://localhost:8000/v1")
model_name = os.getenv("VERIF_MODEL_NAME", "models/IF-Verifier-7B")  # æˆ– "QwQ-32B-Preview"
api_model = APIModel(api_url, model_name)

_global_sampler = None


def _select_backend() -> str:
    backend = os.getenv("VERIF_LLM_BACKEND", "").strip().lower()
    if backend:
        if backend in {"vllm", "sampler", "grader"} and VLLMSampler is not None:
            return "vllm"
        if backend in {"openai", "api", "apimodel", "remote"}:
            return "openai"
    if VLLMSampler is not None and os.getenv("VLLM_BASE_URL"):
        return "vllm"
    return "openai"


def _get_sampler() -> "VLLMSampler":
    global _global_sampler
    if _global_sampler is None:
        max_tokens = int(os.getenv("VERIF_LLM_MAX_TOKENS", "4096"))
        temperature = float(os.getenv("VERIF_LLM_TEMPERATURE", "0"))
        _global_sampler = VLLMSampler(
            max_tokens=max_tokens,
            temperature=temperature,
            enable_thinking=False,
            filter_think_tags=True,
        )
    return _global_sampler


def generate_chat(messages, max_tokens=1280, temperature=0.0):
    backend = _select_backend()
    if backend == "vllm":
        sampler = _get_sampler()
        response = sampler(messages).response_text
        return (response or "").strip()
    response = api_model.generate_chat(messages, max_tokens, temperature)
    return response.strip()


def extract_chat(messages, max_tokens=128, temperature=0.0):
    backend = _select_backend()
    if backend == "vllm":
        sampler = _get_sampler()
        response = sampler(messages).response_text
        return (response or "").strip()
    response = api_model.generate_chat(messages, max_tokens, temperature)
    return response.strip()


prompt_template = """
è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦æ»¡è¶³ç»™å®šçš„çº¦æŸï¼Œä»…å›žç­”æ˜¯æˆ–å¦ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚


åŽŸå§‹æŒ‡ä»¤ï¼š$I$

æ–‡æœ¬ï¼š$R$

çº¦æŸï¼š$C$

åŽŸå§‹æŒ‡ä»¤æè¿°äº†åŸºæœ¬çš„ä»»åŠ¡ä¿¡æ¯ï¼Œç»™å®šçš„çº¦æŸä»‹ç»äº†åº”è¯¥æ»¡è¶³çš„å…·ä½“çš„ä¸€ä¸ªçº¦æŸã€‚
è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦æ»¡è¶³ç»™å®šçš„è¿™ä¸ªçº¦æŸï¼ˆä»…ä»…åˆ¤æ–­æ˜¯å¦æ»¡è¶³ç»™å®šçš„çº¦æŸï¼‰ï¼Œä»…å›žç­”æ˜¯æˆ–å¦ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚
"""


def llm_judge(instruction, response, constraint):
    if isinstance(response, list):
        response = "\n\n".join(response)
    prompt = prompt_template.replace("$R$", response).replace("$C$", constraint).replace("$I", instruction)
    data = [
        {"role": "user", "content": prompt}
    ]
    response = generate_chat(data)
    print(response)
    print(response[0])
    return response[0] == "æ˜¯"


def llm_extract(instruction, response, specific_prompt):
    prompt_suffix = "\nè¯·ç›´æŽ¥è¾“å‡ºæ–‡æœ¬ä¸­çš„åŽŸæ–‡ä¿¡æ¯ï¼Œä¸è¦æ”¹å†™ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„ä¿¡æ¯ã€‚"

    prompt = f"æ–‡æœ¬ï¼š{response}\næŠ½å–è¦æ±‚ï¼š{specific_prompt}" + prompt_suffix
    data = [
        {"role": "user", "content": prompt}
    ]
    response = extract_chat(data, max_tokens=1024)
    return response


def extract_score(text):
    if not text:
        return None
    match = re.search(r"\[\[\s*([01])\s*\]\]", text)
    if match:
        return int(match.group(1))
    # Fallback: accept a leading 0/1 token if brackets are missing.
    match = re.search(r"^\s*([01])\b", text)
    if match:
        return int(match.group(1))
    # Fallback for yes/no in Chinese if it's unambiguous.
    if "æ˜¯" in text and "å¦" not in text:
        return 1
    if "å¦" in text and "æ˜¯" not in text:
        return 0
    return None


def llm_score(instruction, response, checkers):
    prompt = f"""
    è¯·åˆ¤æ–­ç»™å®šçš„å›žå¤æ˜¯å¦éµå¾ªæŒ‡ä»¤ä¸­çš„çº¦æŸï¼Œæ¯”å¦‚é•¿åº¦ã€é£Žæ ¼ã€æ ¼å¼ç­‰çº¦æŸã€‚
    
    [æŒ‡ä»¤]
    {instruction}

    [å›žå¤]
    {response}

    [çº¦æŸ]
    {checkers}

    è¯·åˆ¤æ–­ç»™å®šçš„å›žå¤æ˜¯å¦éµå¾ªæŒ‡ä»¤ä¸­çš„çº¦æŸï¼Œæ¯”å¦‚é•¿åº¦ã€é£Žæ ¼ã€æ ¼å¼ç­‰çº¦æŸã€‚
    è¯·åœ¨å›žç­”çš„æœ€å¼€å§‹ç”¨[[score]]æ ¼å¼è¾“å‡ºä½ çš„åˆ†æ•°ã€‚
    å¦‚æžœéµå¾ªæ‰€æœ‰çš„çº¦æŸï¼Œè¯·è¾“å‡º[[1]]ï¼Œå¦åˆ™è¾“å‡º[[0]]
    """
    data = [
        {"role": "user", "content": prompt}
    ]
    max_retries = int(os.getenv("VERIF_LLM_MAX_RETRIES", "2"))
    backoff_s = float(os.getenv("VERIF_LLM_RETRY_BACKOFF", "0"))

    strict_prompt = prompt + "\n\nè¯·åªè¾“å‡º[[0]]æˆ–[[1]]ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"
    messages = data

    for attempt in range(max_retries + 1):
        response = generate_chat(messages, max_tokens=4096)
        score = extract_score(response)
        if score is not None:
            return score
        # Retry with a stricter format instruction.
        messages = [{"role": "user", "content": strict_prompt}]
        if backoff_s > 0:
            time.sleep(backoff_s * (attempt + 1))
    return 0



if __name__ == "__main__":
    result = llm_judge(
        "What is the speed of light, and how does it compare to the speed of sound in a vacuum? Please answer with a tone of excitement and wonder.The word 'light' should appear at least 3 times, and your response should contain exactly 3 sentences.",
        "Oh, the speed of light is a mind-blowing marvel of the universe, traveling at a staggering 299,792,458 meters per second (m/s)! ðŸŒŸ In comparison, the speed of sound in a vacuum is non-existent because sound needs a medium to travel, whereas light races through the void with unparalleled grace and swiftness. Imagine the thrill of light zooming across the cosmos, effortlessly outpacing any sound, and illuminating the mysteries of space with its incredible speed!",
        "Your response should contain exactly 3 sentences",
    )
    print(result)
    result_score = llm_score(
        "What is the speed of light, and how does it compare to the speed of sound in a vacuum? Please answer with a tone of excitement and wonder.The word 'light' should appear at least 3 times, and your response should contain exactly 3 sentences.",
        "Oh, the speed of light is a mind-blowing marvel of the universe, traveling at a staggering 299,792,458 meters per second (m/s)! ðŸŒŸ In comparison, the speed of sound in a vacuum is non-existent because sound needs a medium to travel, whereas light races through the void with unparalleled grace and swiftness. Imagine the thrill of light zooming across the cosmos, effortlessly outpacing any sound, and illuminating the mysteries of space with its incredible speed!",
        "Your response should contain exactly 3 sentences",
    )
    print(result_score)

    
