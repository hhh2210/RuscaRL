import os
import re

from .build_model import APIModel

try:
    from health_bench.vllm_sampler import VLLMSampler
except Exception:
    VLLMSampler = None


api_url = os.getenv("VERIF_API_URL", "http://localhost:8000/v1")
model_name = os.getenv("VERIF_MODEL_NAME", "models/IF-Verifier-7B")  # æˆ– "QwQ-32B-Preview"
api_model = APIModel(api_url, model_name)

_global_sampler = None


def _select_backend() -> str:
    backend = os.getenv("VERIF_LLM_BACKEND", "").strip().lower()
    if backend:
        if backend in {"vllm", "sampler", "grader"} and VLLMSampler is not None:
            return "vllm"
        if backend in {"openai", "api", "apimodel", "remote"}:
            return "openai"
    if VLLMSampler is not None and os.getenv("VLLM_BASE_URL"):
        return "vllm"
    return "openai"


def _get_sampler() -> "VLLMSampler":
    global _global_sampler
    if _global_sampler is None:
        max_tokens = int(os.getenv("VERIF_LLM_MAX_TOKENS", "4096"))
        temperature = float(os.getenv("VERIF_LLM_TEMPERATURE", "0"))
        _global_sampler = VLLMSampler(
            max_tokens=max_tokens,
            temperature=temperature,
            enable_thinking=False,
            filter_think_tags=True,
        )
    return _global_sampler


def generate_chat(messages, max_tokens=1280, temperature=0.0):
    backend = _select_backend()
    if backend == "vllm":
        sampler = _get_sampler()
        response = sampler(messages).response_text
        return (response or "").strip()
    response = api_model.generate_chat(messages, max_tokens, temperature)
    return response.strip()


def extract_chat(messages, max_tokens=128, temperature=0.0):
    backend = _select_backend()
    if backend == "vllm":
        sampler = _get_sampler()
        response = sampler(messages).response_text
        return (response or "").strip()
    response = api_model.generate_chat(messages, max_tokens, temperature)
    return response.strip()


prompt_template = """
è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦æ»¡è¶³ç»™å®šçš„çº¦æŸï¼Œä»…å›ç­”æ˜¯æˆ–å¦ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚


åŸå§‹æŒ‡ä»¤ï¼š$I$

æ–‡æœ¬ï¼š$R$

çº¦æŸï¼š$C$

åŸå§‹æŒ‡ä»¤æè¿°äº†åŸºæœ¬çš„ä»»åŠ¡ä¿¡æ¯ï¼Œç»™å®šçš„çº¦æŸä»‹ç»äº†åº”è¯¥æ»¡è¶³çš„å…·ä½“çš„ä¸€ä¸ªçº¦æŸã€‚
è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦æ»¡è¶³ç»™å®šçš„è¿™ä¸ªçº¦æŸï¼ˆä»…ä»…åˆ¤æ–­æ˜¯å¦æ»¡è¶³ç»™å®šçš„çº¦æŸï¼‰ï¼Œä»…å›ç­”æ˜¯æˆ–å¦ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚
"""


def llm_judge(instruction, response, constraint):
    if isinstance(response, list):
        response = "\n\n".join(response)
    prompt = prompt_template.replace("$R$", response).replace("$C$", constraint).replace("$I", instruction)
    data = [
        {"role": "user", "content": prompt}
    ]
    response = generate_chat(data)
    print(response)
    print(response[0])
    return response[0] == "æ˜¯"


def llm_extract(instruction, response, specific_prompt):
    prompt_suffix = "\nè¯·ç›´æ¥è¾“å‡ºæ–‡æœ¬ä¸­çš„åŸæ–‡ä¿¡æ¯ï¼Œä¸è¦æ”¹å†™ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„ä¿¡æ¯ã€‚"

    prompt = f"æ–‡æœ¬ï¼š{response}\næŠ½å–è¦æ±‚ï¼š{specific_prompt}" + prompt_suffix
    data = [
        {"role": "user", "content": prompt}
    ]
    response = extract_chat(data, max_tokens=1024)
    return response


def extract_score(text):
    match = re.search(r'\[\[(\d+)\]\]', text)
    try:
        return int(match.group(1))
    except:
        return 0


def llm_score(instruction, response, checkers):
    prompt = f"""
    è¯·åˆ¤æ–­ç»™å®šçš„å›å¤æ˜¯å¦éµå¾ªæŒ‡ä»¤ä¸­çš„çº¦æŸï¼Œæ¯”å¦‚é•¿åº¦ã€é£æ ¼ã€æ ¼å¼ç­‰çº¦æŸã€‚
    
    [æŒ‡ä»¤]
    {instruction}

    [å›å¤]
    {response}

    [çº¦æŸ]
    {checkers}

    è¯·åˆ¤æ–­ç»™å®šçš„å›å¤æ˜¯å¦éµå¾ªæŒ‡ä»¤ä¸­çš„çº¦æŸï¼Œæ¯”å¦‚é•¿åº¦ã€é£æ ¼ã€æ ¼å¼ç­‰çº¦æŸã€‚
    è¯·åœ¨å›ç­”çš„æœ€å¼€å§‹ç”¨[[score]]æ ¼å¼è¾“å‡ºä½ çš„åˆ†æ•°ã€‚
    å¦‚æœéµå¾ªæ‰€æœ‰çš„çº¦æŸï¼Œè¯·è¾“å‡º[[1]]ï¼Œå¦åˆ™è¾“å‡º[[0]]
    """
    data = [
        {"role": "user", "content": prompt}
    ]
    response = generate_chat(data, max_tokens=4096)
    score = extract_score(response)
    return score



if __name__ == "__main__":
    result = llm_judge(
        "What is the speed of light, and how does it compare to the speed of sound in a vacuum? Please answer with a tone of excitement and wonder.The word 'light' should appear at least 3 times, and your response should contain exactly 3 sentences.",
        "Oh, the speed of light is a mind-blowing marvel of the universe, traveling at a staggering 299,792,458 meters per second (m/s)! ğŸŒŸ In comparison, the speed of sound in a vacuum is non-existent because sound needs a medium to travel, whereas light races through the void with unparalleled grace and swiftness. Imagine the thrill of light zooming across the cosmos, effortlessly outpacing any sound, and illuminating the mysteries of space with its incredible speed!",
        "Your response should contain exactly 3 sentences",
    )
    print(result)
    result_score = llm_score(
        "What is the speed of light, and how does it compare to the speed of sound in a vacuum? Please answer with a tone of excitement and wonder.The word 'light' should appear at least 3 times, and your response should contain exactly 3 sentences.",
        "Oh, the speed of light is a mind-blowing marvel of the universe, traveling at a staggering 299,792,458 meters per second (m/s)! ğŸŒŸ In comparison, the speed of sound in a vacuum is non-existent because sound needs a medium to travel, whereas light races through the void with unparalleled grace and swiftness. Imagine the thrill of light zooming across the cosmos, effortlessly outpacing any sound, and illuminating the mysteries of space with its incredible speed!",
        "Your response should contain exactly 3 sentences",
    )
    print(result_score)

    
